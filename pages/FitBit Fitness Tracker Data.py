import streamlit as st
import numpy as np
import pandas as pd
import datetime as dt
import seaborn as sns
import matplotlib.pyplot as plt
import altair as alt
import sklearn
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import os
os.environ["OMP_NUM_THREADS"] = '1'


st.header("Fitbit Fitness Tracker Data")
st.write("A use case example of using the Fitbit Fitness Tracker Data from kaggle.")

tab1, tab2, tab3, tab4 = st.tabs(["Dashboard", "EDA", "Clustering","About"])

with tab1:
    daily_activity = pd.read_csv("data/dailyActivity_merged.csv")
    sleep = pd.read_csv("data/sleepDay_merged.csv")



    ############
    st.header("Fitbit Users Daily Activity Leaderboard")
    st.write("This is a Dashboard showing a summary of all users activity and trends:")
    st.write("Steps Taken:")






    #st.write(df2)
    c1=alt.Chart(daily_activity).mark_bar().encode(
        x= alt.X("TotalSteps",  bin=alt.Bin(maxbins=40)),
        y='count()'
    )

    st.altair_chart(c1, use_container_width=True)


    st.write("Calories Burnt:")
    c2=alt.Chart(daily_activity).mark_bar().encode(
        x= alt.X("Calories",  bin=alt.Bin(maxbins=40)),
        y='count()'
    )

    st.altair_chart(c2, use_container_width=True)


    st.write("Sedentary Minutes:")
    c3=alt.Chart(daily_activity).mark_bar().encode(
        x= alt.X("SedentaryMinutes",  bin=alt.Bin(maxbins=20)),
        y='count()'
    )

    st.altair_chart(c3, use_container_width=True)

    st.write("Total Time In Bed:")

    c4=alt.Chart(sleep).mark_bar().encode(
        x= alt.X("TotalTimeInBed",  bin=alt.Bin(maxbins=10)),
        y='count()'
    )

    st.altair_chart(c4, use_container_width=True)


   

with tab2:
    st.header("Fitbit Exploritory Data Analysis")
    st.write("Quick EDA to understand the data we have:")

    daily_activity = pd.read_csv("data/dailyActivity_merged.csv")
    st.write("Base data")

    st.write(daily_activity)
    st.write("Looking for any null values, there is no missing data")
    st.write(daily_activity.isnull().sum())
    
    unique_id = len(pd.unique(daily_activity["Id"]))
    st.write("### Number of unique IDs(users): " + str(unique_id))

    fig, ax = plt.subplots()

    st.write("# Correlation Matrix")
    sns.heatmap(daily_activity.corr(), ax=ax)
    st.write(fig)
    st.write("In general we see that more active people have more steps and more sedentary people dont have alot of activity which is pretty obvious.")
    



with tab3:
    
    df = pd.read_csv("data/dailyActivity_merged.csv")
    sleep = pd.read_csv("data/sleepDay_merged.csv")

    st.header("Clustering the Fitbit Dataset")
    st.write("Determining the number of clusters using the elbow method")
    x = df.iloc[:,2:]
    wcss = []
    for i in range(1,11):
        model = KMeans(n_clusters = i, init = "k-means++")
        model.fit(x)
        wcss.append(model.inertia_)
    fig = plt.figure() 
    #plt.figure(figsize=(10,10))
    plt.plot(range(1,11), wcss)
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    st.pyplot(fig)

    
 
    st.write("Since the data is multidimensional we will use PCA to view the data easier.")
    pca = PCA(2)
    data = pca.fit_transform(x)
    plt.figure(figsize=(10,10))
    var = np.round(pca.explained_variance_ratio_*100, decimals = 1)
    lbls = [str(x) for x in range(1,len(var)+1)]
    fig2 = plt.figure() 
    plt.bar(x=range(1,len(var)+1), height = var, tick_label = lbls)
    st.pyplot(fig2)
    st.write("variance between pc1 and pc2, We see that pc1 covers for almost most of it so we will only be using 2 Principle components")



    # model2 = KMeans(n_clusters = 3, init = "k-means++")
    # y2 = model2.fit_predict(x)
    # centers = np.array(model2.cluster_centers_)
    st.write("with the elbow method above we see that either 2 or 3 is best")
    cluster_number = st.slider('How many clusters do you want to use?', 0, 10, 2)
    fig3 = plt.figure() 
    model = KMeans(n_clusters = cluster_number, init = "k-means++")
    label = model.fit_predict(data)
    
    #plt.figure(figsize=(10,10))
    uniq = np.unique(label)
    for i in uniq:
        plt.scatter(data[label == i , 0] , data[label == i , 1] , label = i)
    
    #plt.scatter(centers[:,0], centers[:,1], marker="x", color='k')
    #This is done to find the centroid for each clusters.
    plt.legend()
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    st.pyplot(fig3)
with tab4:
    st.header("About this dataset")
    st.write("the original dataset can be found here:")
    link = '[Fitbit Dataset](https://www.kaggle.com/datasets/arashnic/fitbit)'
    st.markdown(link, unsafe_allow_html=True)
    st.write("## Content")
    st.write("This dataset generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016-05.12.2016. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring. Individual reports can be parsed by export session ID (column A) or timestamp (column B). Variation between output represents use of different types of Fitbit trackers and individual tracking behaviors / preferences.")




